{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ceb274b-d7c1-459c-9b06-2682fefc386c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794bd707-e852-46c2-bf5b-7343ad4b2453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FakeDataSourceReader(DataSourceReader):\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema: StructType = schema\n",
    "        self.options = options\n",
    "\n",
    "    def read(self, partition):\n",
    "        # Library imports must be within the method.\n",
    "        from faker import Faker\n",
    "        fake = Faker()\n",
    "\n",
    "        # Every value in this `self.options` dictionary is a string.\n",
    "        num_rows = int(self.options.get(\"numRows\", 3))\n",
    "        for _ in range(num_rows):\n",
    "            row = []\n",
    "            for field in self.schema.fields:\n",
    "                value = getattr(fake, field.name)()\n",
    "                row.append(value)\n",
    "            yield tuple(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406baea9-92b7-4006-8c41-7dbf1f6e7706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "class FakeDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    An example data source for batch query using the `faker` library.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"fake\"\n",
    "\n",
    "    def schema(self):\n",
    "        return \"name string, date string, zipcode string, state string\"\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return FakeDataSourceReader(schema, self.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f64f20-4f63-453c-b194-72e534014f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(FakeDataSource)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69180f50-11ea-4590-b787-88bd525cfc82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"fake\").load()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782cfbcd-e6d0-4661-848b-9ca849c654ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"fake\").schema(\"name string, company string\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a1d4d8-cf58-456c-82bd-882bc0c127b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"fake\").option(\"numRows\", 5).load().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "faker_source_data.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
